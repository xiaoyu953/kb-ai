Heima-AI-RAG - 智能文档问答系统（支持 PDF 引用溯源 + 防幻觉）
基于 Spring AI + DeepSeek 大模型 + Qdrant 开发的企业级 RAG 智能问答项目，实现「上传 PDF/TXT 文档 → 自然语言提问 → AI
精准回答 + 文档来源标注」全闭环，无幻觉、可溯源、部署简单，完美适配企业知识库 / 员工手册问答场景。
✨ 核心功能
✅ 基础智能对话：对接 DeepSeek 大模型，原生支持多轮会话，无文档也可正常聊天
✅ 文档精准问答：上传 PDF / 纯文本文件，AI 严格基于文档内容作答，拒绝编造信息
✅ 精准引用溯源：回答末尾自动标注【来源: xxx.pdf, p.x】，信息可追溯，可信度拉满
✅ 强力防幻觉机制：文档无相关信息时，统一回复「我不知道」，彻底杜绝虚假生成
✅ 智能文本切片：基于语义的分块策略，保留上下文连贯性，大幅提升检索准确率
✅ 性能极致优化：Redis 缓存检索结果，相同问题秒级响应，降低模型调用成本
✅ 多格式兼容：完美支持 PDF、TXT 文档解析，支持大文件流式处理，无内存溢出风险
✅ 一键化部署：Docker Compose 一行命令启动所有服务，无需复杂环境配置，本地 / 云服务器通用
🛠️ 技术栈选型
后端核心
基础框架：Spring Boot 3.5.x + Spring AI 1.1.5
大语言模型：DeepSeek-chat（中文友好、响应快、性价比高）
向量嵌入模型：HuggingFace 多语言模型（本地部署、免费无调用成本、完美支持中文）
向量数据库：Qdrant（轻量级、高性能、支持持久化、适配中小规模 RAG 场景）
缓存组件：Redis（缓存问答结果，提升响应速度）
文档解析：Apache Tika（解决 PDF 中文乱码、精准提取文本内容）
文本处理：Spring AI 原生语义切片工具
部署相关
容器化：Docker + Docker Compose
环境要求：通用环境，无特殊依赖
📊 系统架构图
查看代码
缓存命中 秒返回

缓存未命中

前端聊天界面

Spring Boot 后端服务

请求分发

纯聊天接口

DeepSeek 大模型

文档问答接口

Redis 缓存查询

Qdrant 向量库检索相似文本

拼接文档上下文+约束Prompt

文档上传接口

PDF/TXT文本解析

智能语义切片

本地文本向量化

缓存命中 秒返回

缓存未命中

前端聊天界面

Spring Boot 后端服务

请求分发

纯聊天接口

DeepSeek 大模型

文档问答接口

Redis 缓存查询

Qdrant 向量库检索相似文本

拼接文档上下文+约束Prompt

文档上传接口

PDF/TXT文本解析

智能语义切片

本地文本向量化

豆包
你的 AI 助手，助力每日工作学习
🚀 快速开始（3 分钟跑通，零基础也能部署）
环境准备
本地安装 Docker 和 Docker Compose（通用环境，Windows/Mac/Linux 均支持）
获取你的 DeepSeek API-KEY：DeepSeek 控制台获取
克隆本项目到本地
一键部署启动
进入项目根目录，新建 .env 文件，写入你的密钥（仅此一步配置）
plaintext
DEEPSEEK_API_KEY=你的DeepSeek-API-KEY
执行一键启动命令，自动构建 + 启动所有服务（后端 + Qdrant+Redis）
bash
运行
docker-compose up -d
服务启动成功后，可访问的地址
后端接口根地址：http://localhost:8080
Qdrant 可视化管理面板：http://localhost:6333/dashboard
功能测试（三步验证完整流程）
第一步：上传文档
bash
运行
curl -F "file=@你的文档路径/公司手册.pdf" http://localhost:8080/rag/upload
第二步：文档智能问答
bash
运行
curl "http://localhost:8080/ai/rag-chat?prompt=年假可以休几天&chatId=test001"
第三步：查看结果
示例返回：员工入职满 1 年可享受 15 天年假，年假可拆分申请，未休年假可累计至次年。【来源：公司手册.pdf, p.5】
📝 核心接口文档
统一返回格式说明
所有接口均返回标准化 JSON 格式，前端对接无适配成本，异常信息统一返回
json
{
"code": 200, // 200=成功 | 400=参数错误 | 500=系统异常
"msg": "操作成功",
"data": "接口返回的核心数据"
}

1. 纯 AI 聊天接口（无文档问答）
   请求方式：GET
   请求地址：/ai/chat
   请求参数：prompt = 你的问题 & chatId = 会话 ID
   说明：基础大模型对话能力，无需上传文档即可使用
2. 文档智能问答接口【核心】
   请求方式：GET
   请求地址：/ai/rag-chat
   请求参数：prompt = 你的问题 & chatId = 会话 ID
   说明：基于已上传的文档内容作答，自动携带文档来源标注，无相关信息则返回「我不知道」
3. 文档上传接口【核心】
   请求方式：POST
   请求地址：/rag/upload
   请求参数：file = 文件流（支持 PDF/TXT 格式，最大 100MB）
   说明：上传后自动完成「文本提取→智能切片→向量化→入库」全流程，无需手动干预
   ⚠️ 常见问题 & 避坑指南（解决 99% 的部署 / 使用问题）
   接口访问失败、服务启动异常：执行 docker-compose ps 查看所有服务状态，确保 3 个服务均为 UP 状态
   DeepSeek 调用报 401 错误：检查.env 文件中的 API-KEY 是否复制完整、无空格，确认已开通 deepseek-chat 模型权限
   向量检索无结果：确保上传文档后再提问，且提问内容与文档相关，中文提问无特殊符号
   PDF 解析乱码：项目已内置中文解析器，无需额外配置，直接上传即可
   云服务器部署后外网无法访问：开放云服务器安全组 8080、6333 端口
   缓存不生效：Redis 服务自动启动，无需手动配置，相同问题间隔短会自动命中缓存
   大文件上传失败：默认支持最大 100MB 文件，满足绝大多数文档场景需求
   ✨ 项目核心亮点（求职面试加分项）
   ✅ 企业级工程化规范：无硬编码、统一异常处理、标准化返回格式，代码可直接落地生产
   ✅ 极致的防幻觉能力：通过 Prompt 强制约束 + 文档内容校验，彻底解决大模型编造信息的痛点
   ✅ 完整的溯源体系：文档元数据精准记录，回答来源可查，满足企业合规性要求
   ✅ 零成本部署：本地嵌入模型无调用费用，DeepSeek 性价比高，个人 / 中小企业均可无压力使用
   ✅ 高度可扩展：基于 Spring AI 抽象接口，可无缝切换大模型 / 向量库，无需修改核心业务代码
   ✅ 高性能：缓存 + 智能切片双重优化，响应速度快，服务器资源占用低
   📄 开源协议
   MIT License
   可自由使用、修改、二次开发、商用，无任何限制。欢迎 Star、Fork，如有优化建议欢迎交流。
   📌 项目说明
   本项目是一套完整的企业级 RAG 落地解决方案，从技术选型到工程实现均对标企业真实开发场景，完美解决大模型「知识过时、信息不准、有幻觉」三大核心痛点。
   项目功能闭环、部署简单、文档齐全，既是入门 Spring AI+RAG 的优质学习项目，也是可直接写进简历、给面试官演示的高含金量作品集。
   补充说明
   演示视频：点击查看项目功能演示
   项目已完成全流程测试，所有功能均可正常使用，如有问题可参考避坑指南解决